{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date_column_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/harbinger/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date_column_name'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 133\u001b[0m\n\u001b[1;32m    126\u001b[0m dataset_info \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    127\u001b[0m     (az_cocci, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_column_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArizona Cocci Case Count\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    128\u001b[0m     (az_pop, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_column_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArizona Population Estimates\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# Add all your datasets here with their respective date column names\u001b[39;00m\n\u001b[1;32m    130\u001b[0m ]\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Analyze all datasets\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m temporal_gaps \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\n\u001b[1;32m    134\u001b[0m     analyze_temporal_gaps(df, date_col, title)\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m df, date_col, title \u001b[38;5;129;01min\u001b[39;00m dataset_info\n\u001b[1;32m    136\u001b[0m ], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Sort by gap percentage\u001b[39;00m\n\u001b[1;32m    139\u001b[0m temporal_gaps \u001b[38;5;241m=\u001b[39m temporal_gaps\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGaps_Percentage\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[1], line 134\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    126\u001b[0m dataset_info \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    127\u001b[0m     (az_cocci, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_column_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArizona Cocci Case Count\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    128\u001b[0m     (az_pop, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_column_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArizona Population Estimates\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# Add all your datasets here with their respective date column names\u001b[39;00m\n\u001b[1;32m    130\u001b[0m ]\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Analyze all datasets\u001b[39;00m\n\u001b[1;32m    133\u001b[0m temporal_gaps \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\n\u001b[0;32m--> 134\u001b[0m     \u001b[43manalyze_temporal_gaps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m df, date_col, title \u001b[38;5;129;01min\u001b[39;00m dataset_info\n\u001b[1;32m    136\u001b[0m ], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Sort by gap percentage\u001b[39;00m\n\u001b[1;32m    139\u001b[0m temporal_gaps \u001b[38;5;241m=\u001b[39m temporal_gaps\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGaps_Percentage\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[1], line 75\u001b[0m, in \u001b[0;36manalyze_temporal_gaps\u001b[0;34m(df, date_column, title, freq)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03mAnalyze temporal gaps in a dataset\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03mReturns DataFrame with gap analysis\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Convert date column to datetime if it's not already\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m df[date_column] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdate_column\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Sort by date\u001b[39;00m\n\u001b[1;32m     78\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msort_values(date_column)\n",
      "File \u001b[0;32m~/miniconda3/envs/harbinger/lib/python3.9/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/harbinger/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date_column_name'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# load all datasets\n",
    "az_cocci = pd.read_csv(\"../../data/processed/arizona_cm_combined_data.csv\")\n",
    "az_pop = pd.read_csv(\"../../data/processed/arizona_pop_est_1980-2023.csv\")\n",
    "az_met = pd.read_csv(\"../../data/processed/Arizona_Weather_Data_Daily_Updates_1994_to_2023.csv\")\n",
    "ca_cocci = pd.read_csv(\"../../data/processed/Cali_Monthly_Cases.csv\")\n",
    "ca_pop = pd.read_csv(\"../../data/processed/California_Population_2000-2023.csv\")\n",
    "ca_met = pd.read_csv(\"../../data/processed/California_Weather_Data_Daily_Updates_2001_to_2022.csv\")\n",
    "az_co = pd.read_csv(\"../../data/processed/arizona_CO_1993_2023.csv\", low_memory=False)\n",
    "az_no2 = pd.read_csv(\"../../data/processed/arizona_NO2_1993_2023.csv\", low_memory=False)\n",
    "az_oz = pd.read_csv(\"../../data/processed/arizona_Ozone_1993_2023.csv\", low_memory=False)\n",
    "az_pm2_5 = pd.read_csv(\"../../data/processed/arizona_PM2.5_1993_2023.csv\", low_memory=False)\n",
    "az_pm2_5_nonref = pd.read_csv(\"../../data/processed/arizona_PM2.5_nonref_1993_2023.csv\", low_memory=False)\n",
    "az_pm10 = pd.read_csv(\"../../data/processed/arizona_PM10_1993_2023.csv\", low_memory=False)\n",
    "az_so2 = pd.read_csv(\"../../data/processed/arizona_SO2_1993_2023.csv\", low_memory=False)\n",
    "az_tsp = pd.read_csv(\"../../data/processed/arizona_TSP_1993_2023.csv\", low_memory=False)\n",
    "ca_co = pd.read_csv(\"../../data/processed/california_CO_2000_2022.csv\", low_memory=False)\n",
    "ca_no2 = pd.read_csv(\"../../data/processed/california_NO2_2000_2022.csv\", low_memory=False)\n",
    "ca_oz = pd.read_csv(\"../../data/processed/california_Ozone_2000_2022.csv\", low_memory=False)\n",
    "ca_pm2_5 = pd.read_csv(\"../../data/processed/california_PM2.5_2000_2022.csv\", low_memory=False)\n",
    "ca_pm2_5_nonref = pd.read_csv(\"../../data/processed/california_PM2.5_nonref_2000_2022.csv\", low_memory=False)\n",
    "ca_pm10 = pd.read_csv(\"../../data/processed/california_PM10_2000_2022.csv\", low_memory=False)\n",
    "ca_so2 = pd.read_csv(\"../../data/processed/california_SO2_2000_2022.csv\", low_memory=False)\n",
    "ca_tsp = pd.read_csv(\"../../data/processed/california_TSP_2000_2022.csv\", low_memory=False)\n",
    "ca_az_air_pollutants = pd.read_csv(\"../../data/processed/air_pollutants_ca_and_az.csv\", low_memory=False)\n",
    "ca_air_pollutants = pd.read_csv(\"../../data/processed/air_pollutants_ca.csv\", low_memory=False)\n",
    "az_air_pollutants = pd.read_csv(\"../../data/processed/air_pollutants_az.csv\", low_memory=False)\n",
    "\n",
    "# list of dataframes and their titles\n",
    "dfs = [az_cocci, az_pop, az_met, ca_cocci, ca_pop, ca_met, \n",
    "       az_co, az_no2, az_oz, az_pm2_5, az_pm2_5_nonref, az_pm10, az_so2, az_tsp,\n",
    "       ca_co, ca_no2, ca_oz, ca_pm2_5, ca_pm2_5_nonref, ca_pm10, ca_so2, ca_tsp, \n",
    "       ca_az_air_pollutants, ca_air_pollutants, az_air_pollutants]\n",
    "\n",
    "dfs_titles = ['Arizona Cocci Case Count', \n",
    "              'Arizona Population Estimates',\n",
    "              'Arizona Weather Data',\n",
    "              'California Cocci Case Count',\n",
    "              'California Population Estimates',\n",
    "              'California Weather Data',\n",
    "              'Arizona CO Data 1993-2023',\n",
    "              'Arizona NO2 Data 1993-2023',\n",
    "              'Arizona Ozone Data 1993-2023',\n",
    "              'Arizona PM2.5 Data 1993-2023',\n",
    "              'Arizona PM2.5 Non-Reference Data 1993-2023',\n",
    "              'Arizona PM10 Data 1993-2023',\n",
    "              'Arizona SO2 Data 1993-2023',\n",
    "              'Arizona TSP Data 1993-2023',\n",
    "              'California CO Data 2000-2022',\n",
    "              'California NO2 Data 2000-2022',\n",
    "              'California Ozone Data 2000-2022',\n",
    "              'California PM2.5 Data 2000-2022',\n",
    "              'California PM2.5 Non-Reference Data 2000-2022',\n",
    "              'California PM10 Data 2000-2022',\n",
    "              'California SO2 Data 2000-2022',\n",
    "              'California TSP Data 2000-2022',\n",
    "              'California and Arizona Air Pollutants',\n",
    "              'California Air Pollutants',\n",
    "              'Arizona Air Pollutants']\n",
    "\n",
    "def analyze_temporal_gaps(df, date_column, title, freq='D'):\n",
    "    \"\"\"\n",
    "    Analyze temporal gaps in a dataset\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - date_column: name of the column containing dates\n",
    "    - title: name of the dataset\n",
    "    - freq: frequency to check for gaps ('D' for daily, 'M' for monthly, etc.)\n",
    "    \n",
    "    Returns DataFrame with gap analysis\n",
    "    \"\"\"\n",
    "    # Convert date column to datetime if it's not already\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    \n",
    "    # Sort by date\n",
    "    df = df.sort_values(date_column)\n",
    "    \n",
    "    # Create complete date range\n",
    "    full_range = pd.date_range(start=df[date_column].min(), \n",
    "                             end=df[date_column].max(),\n",
    "                             freq=freq)\n",
    "    \n",
    "    # Find missing dates\n",
    "    existing_dates = set(df[date_column])\n",
    "    missing_dates = [date for date in full_range if date not in existing_dates]\n",
    "    \n",
    "    # Calculate basic statistics\n",
    "    stats = {\n",
    "        'Dataset': title,\n",
    "        'Start_Date': df[date_column].min(),\n",
    "        'End_Date': df[date_column].max(),\n",
    "        'Total_Days': len(full_range),\n",
    "        'Days_With_Data': len(existing_dates),\n",
    "        'Missing_Days': len(missing_dates),\n",
    "        'Gaps_Percentage': (len(missing_dates) / len(full_range)) * 100,\n",
    "        'Longest_Gap': 0,\n",
    "        'Gap_Start_Date': None,\n",
    "        'Gap_End_Date': None\n",
    "    }\n",
    "    \n",
    "    # Find longest gap\n",
    "    if missing_dates:\n",
    "        missing_dates = sorted(missing_dates)\n",
    "        gaps = []\n",
    "        gap_start = missing_dates[0]\n",
    "        prev_date = missing_dates[0]\n",
    "        \n",
    "        for date in missing_dates[1:]:\n",
    "            if (date - prev_date).days > 1:\n",
    "                gaps.append((gap_start, prev_date))\n",
    "                gap_start = date\n",
    "            prev_date = date\n",
    "        gaps.append((gap_start, missing_dates[-1]))\n",
    "        \n",
    "        # Find longest gap\n",
    "        longest_gap = max(gaps, key=lambda x: (x[1] - x[0]).days)\n",
    "        stats['Longest_Gap'] = (longest_gap[1] - longest_gap[0]).days\n",
    "        stats['Gap_Start_Date'] = longest_gap[0]\n",
    "        stats['Gap_End_Date'] = longest_gap[1]\n",
    "    \n",
    "    return pd.DataFrame([stats])\n",
    "\n",
    "# Create list of datasets and their date columns\n",
    "dataset_info = [\n",
    "    (az_cocci, 'date_column_name', 'Arizona Cocci Case Count'),\n",
    "    (az_pop, 'date_column_name', 'Arizona Population Estimates'),\n",
    "    # Add all your datasets here with their respective date column names\n",
    "]\n",
    "\n",
    "# Analyze all datasets\n",
    "temporal_gaps = pd.concat([\n",
    "    analyze_temporal_gaps(df, date_col, title)\n",
    "    for df, date_col, title in dataset_info\n",
    "], ignore_index=True)\n",
    "\n",
    "# Sort by gap percentage\n",
    "temporal_gaps = temporal_gaps.sort_values('Gaps_Percentage', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harbinger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
